{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Function\n",
    "def softmax(u):\n",
    "    return np.exp(u) / np.sum(np.exp(u),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Entropy Loss Function \n",
    "def multiclass_cross_entropy(p,q):\n",
    "    return -np.vdot(p,np.log(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective Function\n",
    "def L(beta,X,y):\n",
    "    N = X.shape[0]\n",
    "    s = 0\n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi = y[i]\n",
    "        \n",
    "        #Prediction Function\n",
    "        u = beta @ xiHat\n",
    "        yi_pred = softmax(u)\n",
    "        \n",
    "#         Calculating the loss\n",
    "        s += multiclass_cross_entropy(yi,yi_pred)\n",
    "    \n",
    "    #Returns the average loss \n",
    "    return s / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Gradient\n",
    "def grad_L(beta,X,y):\n",
    "    N = X.shape[0]\n",
    "    grad = 0\n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi = y[i]\n",
    "        u = beta @ xiHat \n",
    "        yi_pred = softmax(u)\n",
    "        \n",
    "        grad = grad + np.outer((softmax(u) - yi), xiHat)\n",
    "        \n",
    "    return grad / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression using Gradient Descent\n",
    "def MultiLogReg_GradientDescent(X,y):\n",
    "    num_iters = 100\n",
    "    alpha = 3\n",
    "    beta = np.zeros((K,d+1))\n",
    "    \n",
    "    L_vals = np.zeros(num_iters)\n",
    "    for t in range(num_iters):\n",
    "        L_vals[t] = L(beta,X,y)\n",
    "        \n",
    "        if t % 20:\n",
    "            print('Loss : '+ str(L_vals[t]))\n",
    "            \n",
    "        beta = beta - alpha * grad_L(beta,X,y)\n",
    "    \n",
    "    return beta, L_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict labels\n",
    "def make_predictions(beta,X):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi_pred = softmax(beta @ xiHat)\n",
    "        \n",
    "        k = np.argmax(yi_pred)\n",
    "        predictions.append(k)\n",
    "        \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the dataset\n",
    "dataset = sk.datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "N = len(y)\n",
    "K = len(np.unique(y))\n",
    "\n",
    "#One-hot Encoding \n",
    "y = pd.get_dummies(y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = X_train.shape[0] #Training Set : Number of rows in X_train\n",
    "N_val = X_train.shape[0] #Test Set : Number of rows in X_train ??? Why X_train\n",
    "d = X_train.shape[1] #Number of columns in X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "X_train = (X_train - np.mean(X_train,axis=0)) / np.std(X_train,axis=0)\n",
    "X_test = (X_test - np.mean(X_train,axis=0)) / np.std(X_train,axis=0) #??? Why X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert Leading 1's to create the augmented matrix \n",
    "X_train = np.insert(X_train,0,1,axis=1)\n",
    "X_test = np.insert(X_test,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.3743215487275355\n",
      "Loss : 0.2915440682140621\n",
      "Loss : 0.2696585302466925\n",
      "Loss : 0.2528041939488977\n",
      "Loss : 0.23883836510999581\n",
      "Loss : 0.22701111214845213\n",
      "Loss : 0.21686873481380634\n",
      "Loss : 0.20807268728334544\n",
      "Loss : 0.20035956402673963\n",
      "Loss : 0.19352387036306212\n",
      "Loss : 0.18740639531942763\n",
      "Loss : 0.1818843518459552\n",
      "Loss : 0.17686285981411146\n",
      "Loss : 0.1722679696667858\n",
      "Loss : 0.16804126282809137\n",
      "Loss : 0.16413584309351856\n",
      "Loss : 0.16051343362839227\n",
      "Loss : 0.15714229641821653\n",
      "Loss : 0.15399574001309682\n",
      "Loss : 0.14828864448333465\n",
      "Loss : 0.14569158531237514\n",
      "Loss : 0.14324502363850872\n",
      "Loss : 0.1409359018519997\n",
      "Loss : 0.1387526669997511\n",
      "Loss : 0.13668504912174584\n",
      "Loss : 0.134723880822531\n",
      "Loss : 0.13286094857495595\n",
      "Loss : 0.13108886885945356\n",
      "Loss : 0.12940098404024578\n",
      "Loss : 0.12779127414042746\n",
      "Loss : 0.12625428157696658\n",
      "Loss : 0.1247850465693898\n",
      "Loss : 0.1233790514181922\n",
      "Loss : 0.12203217221139662\n",
      "Loss : 0.12074063679433439\n",
      "Loss : 0.11950098805200347\n",
      "Loss : 0.11831005172152781\n",
      "Loss : 0.11716490808578958\n",
      "Loss : 0.11500144584150895\n",
      "Loss : 0.11397834986290964\n",
      "Loss : 0.11299145484901765\n",
      "Loss : 0.11203879157376068\n",
      "Loss : 0.11111853195543248\n",
      "Loss : 0.11022897666126753\n",
      "Loss : 0.10936854399232004\n",
      "Loss : 0.1085357598970406\n",
      "Loss : 0.10772924898218819\n",
      "Loss : 0.10694772640694436\n",
      "Loss : 0.1061899905608061\n",
      "Loss : 0.10545491643842811\n",
      "Loss : 0.10474144963541375\n",
      "Loss : 0.104048600898372\n",
      "Loss : 0.1033754411706142\n",
      "Loss : 0.10272109708183454\n",
      "Loss : 0.10208474683617502\n",
      "Loss : 0.10146561645833692\n",
      "Loss : 0.10086297636199634\n",
      "Loss : 0.09970445202966491\n",
      "Loss : 0.09914730358341269\n",
      "Loss : 0.09860411193003175\n",
      "Loss : 0.09807432719880005\n",
      "Loss : 0.09755742853316378\n",
      "Loss : 0.09705292219634544\n",
      "Loss : 0.09656033982323915\n",
      "Loss : 0.09607923680561713\n",
      "Loss : 0.09560919079896595\n",
      "Loss : 0.09514980034042511\n",
      "Loss : 0.09470068356832408\n",
      "Loss : 0.09426147703473171\n",
      "Loss : 0.09383183460324697\n",
      "Loss : 0.09341142642499321\n",
      "Loss : 0.09299993798643111\n",
      "Loss : 0.09259706922319402\n",
      "Loss : 0.09220253369467622\n",
      "Loss : 0.09181605781457836\n",
      "Loss : 0.09143738013304183\n",
      "Loss : 0.09070243027082349\n",
      "Loss : 0.09034569005679312\n",
      "Loss : 0.0899958108409314\n",
      "Loss : 0.08965258263283868\n",
      "Loss : 0.08931580415412393\n",
      "Loss : 0.08898528238737714\n",
      "Loss : 0.08866083215292293\n",
      "Loss : 0.08834227571137919\n",
      "Loss : 0.08802944239020702\n",
      "Loss : 0.08772216823257896\n",
      "Loss : 0.08742029566702593\n",
      "Loss : 0.08712367319644324\n",
      "Loss : 0.08683215510514539\n",
      "Loss : 0.08654560118275884\n",
      "Loss : 0.08626387646383546\n",
      "Loss : 0.08598685098215181\n",
      "Loss : 0.08571439953873701\n",
      "Loss : 0.08544640148274192\n",
      "Loss : 0.08518274050432853\n"
     ]
    }
   ],
   "source": [
    "beta, L_vals = MultiLogReg_GradientDescent(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f85760a3310>]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = make_predictions(beta,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
