{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Function\n",
    "def softmax(u):\n",
    "    return np.exp(u) / np.sum(np.exp(u),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Entropy Loss Function \n",
    "def multiclass_cross_entropy(p,q):\n",
    "    return -np.vdot(p,np.log(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective Function\n",
    "def L(beta,X,y):\n",
    "    N = X.shape[0]\n",
    "    s = 0\n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi = y[i]\n",
    "        \n",
    "        #Prediction Function\n",
    "        u = beta @ xiHat\n",
    "        yi_pred = softmax(u)\n",
    "        \n",
    "        #Calculating the loss\n",
    "        s += multiclass_cross_entropy(yi,yi_pred)\n",
    "    \n",
    "    #Returns the average loss \n",
    "    return s / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform Multiclass Logistic Regression using Stochastic Gradient Descent\n",
    "def multiLogReg_SGD(X,y,batch,a_val,ep): #(Training, Testing, Batch_Size, Alpha, Epoch)\n",
    "    num_epochs = ep\n",
    "    alpha = a_val\n",
    "    N, d = X.shape\n",
    "    X = np.insert(X,0,1,axis=1) #Inserting Leading 1's to create augmented matrix\n",
    "    K = y.shape[1] #Setting K to the number of classes in the test set\n",
    "    \n",
    "    batch_size = batch #Initialising batch_size \n",
    "    \n",
    "    beta = np.zeros((K,d+1)) #Initialising beta vector\n",
    "    L_vals = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        L_val = L(beta,X,y) #Calculate the loss for every epoch\n",
    "        L_vals.append(L_val) #Add the losses into L_vals\n",
    "        \n",
    "        print('| Epoch : ' + str(epoch) + ' | Loss : ' + str(L_val))\n",
    "        \n",
    "        prm = np.random.permutation(N) #A random permutation of size N \n",
    "        \n",
    "        batch_idx = 0\n",
    "        for start_idx in range(0,N,batch_size): #Starting from 0, to length N in steps of the batch size\n",
    "            \n",
    "            stop_idx = start_idx + batch_size \n",
    "            stop_idx = min(stop_idx, N)\n",
    "            \n",
    "            num_examples_in_batch = stop_idx - start_idx \n",
    "        \n",
    "            for i in prm[start_idx:stop_idx]:\n",
    "                xiHat = X[i]\n",
    "                yi = y[i]\n",
    "                u = beta @ xiHat\n",
    "                \n",
    "                #Prediction Function\n",
    "                yi_pred = softmax(u)\n",
    "                \n",
    "                #Calculating gradient \n",
    "                grad = np.outer(yi_pred - yi, xiHat)\n",
    "\n",
    "            #Divide gradient by number of examples in batch \n",
    "            grad = grad / num_examples_in_batch\n",
    "            beta = beta - alpha*grad \n",
    "            \n",
    "            batch_idx += 1\n",
    "            \n",
    "    \n",
    "    return beta, L_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict labels\n",
    "def predictLabels(beta,X):\n",
    "    X = np.insert(X,0,1,axis = 1)\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi_pred = softmax(beta @ xiHat)\n",
    "        \n",
    "        k = np.argmax(yi_pred)\n",
    "        predictions.append(k)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate accuracy\n",
    "def accuracy_score(predicted_labels,truth_labels):\n",
    "    \n",
    "    yi_pred = np.array(predicted_labels)\n",
    "    truth = np.array(truth_labels)\n",
    "    num_correct = sum(p == t for p, t in zip(yi_pred, truth)) \n",
    "    accuracy = num_correct/truth_labels.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the training set \n",
    "N_train, numRows, numCols = X_train.shape\n",
    "X_train = np.reshape(X_train,(N_train,numRows*numCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encoding\n",
    "y_train = pd.get_dummies(y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch : 0 | Loss : 2.3025850929954172\n",
      "| Epoch : 1 | Loss : 0.7478375617837253\n",
      "| Epoch : 2 | Loss : 0.575040907389319\n",
      "| Epoch : 3 | Loss : 0.507214506652957\n",
      "| Epoch : 4 | Loss : 0.4688793489334052\n",
      "| Epoch : 5 | Loss : 0.44434912608452726\n",
      "| Epoch : 6 | Loss : 0.42621315180443736\n",
      "| Epoch : 7 | Loss : 0.4109275839995608\n",
      "| Epoch : 8 | Loss : 0.39901411138491233\n",
      "| Epoch : 9 | Loss : 0.3911669494263595\n",
      "| Epoch : 10 | Loss : 0.38406908033427917\n",
      "| Epoch : 11 | Loss : 0.3747312106627982\n",
      "| Epoch : 12 | Loss : 0.3693341036614135\n",
      "| Epoch : 13 | Loss : 0.3636067732590776\n",
      "| Epoch : 14 | Loss : 0.35898253871933455\n",
      "| Epoch : 15 | Loss : 0.35554483817212135\n",
      "| Epoch : 16 | Loss : 0.3525378087765864\n",
      "| Epoch : 17 | Loss : 0.3485334629427758\n",
      "| Epoch : 18 | Loss : 0.3444823664924098\n",
      "| Epoch : 19 | Loss : 0.3413492470289609\n",
      "| Epoch : 20 | Loss : 0.3386846220285912\n",
      "| Epoch : 21 | Loss : 0.33732613891612\n",
      "| Epoch : 22 | Loss : 0.3343641516373868\n",
      "| Epoch : 23 | Loss : 0.3348793184604953\n",
      "| Epoch : 24 | Loss : 0.32955142623426287\n",
      "| Epoch : 25 | Loss : 0.3289198308107813\n",
      "| Epoch : 26 | Loss : 0.3266709500002594\n",
      "| Epoch : 27 | Loss : 0.3259241719946902\n",
      "| Epoch : 28 | Loss : 0.32341473249517994\n",
      "| Epoch : 29 | Loss : 0.3226584466042396\n",
      "| Epoch : 30 | Loss : 0.32067661357465604\n",
      "| Epoch : 31 | Loss : 0.3189262605345473\n",
      "| Epoch : 32 | Loss : 0.31925763001927693\n",
      "| Epoch : 33 | Loss : 0.31681213133442254\n",
      "| Epoch : 34 | Loss : 0.3181684843878479\n",
      "| Epoch : 35 | Loss : 0.3160352142447122\n",
      "| Epoch : 36 | Loss : 0.31358475963342086\n",
      "| Epoch : 37 | Loss : 0.3122492875085403\n",
      "| Epoch : 38 | Loss : 0.31036336746609666\n",
      "| Epoch : 39 | Loss : 0.31081538473669157\n",
      "| Epoch : 40 | Loss : 0.30870427613101964\n",
      "| Epoch : 41 | Loss : 0.30821320295297056\n",
      "| Epoch : 42 | Loss : 0.3068438485754055\n",
      "| Epoch : 43 | Loss : 0.30650469304077205\n",
      "| Epoch : 44 | Loss : 0.3055330549072814\n",
      "| Epoch : 45 | Loss : 0.305248140025417\n",
      "| Epoch : 46 | Loss : 0.3048651422531071\n",
      "| Epoch : 47 | Loss : 0.30379950804886713\n",
      "| Epoch : 48 | Loss : 0.30307640543239006\n",
      "| Epoch : 49 | Loss : 0.3021047487442962\n",
      "| Epoch : 50 | Loss : 0.30166903147478247\n",
      "| Epoch : 51 | Loss : 0.3014803445305876\n",
      "| Epoch : 52 | Loss : 0.3002100443445028\n",
      "| Epoch : 53 | Loss : 0.30012333025163096\n",
      "| Epoch : 54 | Loss : 0.300644667415156\n",
      "| Epoch : 55 | Loss : 0.2988497276904872\n",
      "| Epoch : 56 | Loss : 0.2969973587903459\n",
      "| Epoch : 57 | Loss : 0.2974092521202329\n",
      "| Epoch : 58 | Loss : 0.2967103616620002\n",
      "| Epoch : 59 | Loss : 0.29576720901353226\n",
      "| Epoch : 60 | Loss : 0.2958585278227695\n",
      "| Epoch : 61 | Loss : 0.2960065364553129\n",
      "| Epoch : 62 | Loss : 0.29513579258295264\n",
      "| Epoch : 63 | Loss : 0.29404142097506575\n",
      "| Epoch : 64 | Loss : 0.29359332562483886\n",
      "| Epoch : 65 | Loss : 0.29435706247636245\n",
      "| Epoch : 66 | Loss : 0.29289738408861565\n",
      "| Epoch : 67 | Loss : 0.29258620556073905\n",
      "| Epoch : 68 | Loss : 0.292368475154728\n",
      "| Epoch : 69 | Loss : 0.2911624812782648\n",
      "| Epoch : 70 | Loss : 0.29159534154694855\n",
      "| Epoch : 71 | Loss : 0.2921768650599624\n",
      "| Epoch : 72 | Loss : 0.28988555689180395\n"
     ]
    }
   ],
   "source": [
    "beta, L_vals = multiLogReg_SGD(X_train,y_train,10,0.01,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the test array\n",
    "N_test = X_test.shape[0]\n",
    "X_test = np.reshape(X_test,(N_test,numRows*numCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions for X_test\n",
    "predictions = predictLabels(beta,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 10 rows of the predictions\n",
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 10 rows of the truth\n",
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy : \n",
    "accuracy_score(predictions,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
