{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Function\n",
    "def softmax(u):\n",
    "    return np.exp(u) / np.sum(np.exp(u),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Entropy Loss Function \n",
    "def multiclass_cross_entropy(p,q):\n",
    "    return -np.vdot(p,np.log(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective Function\n",
    "def L(beta,X,y):\n",
    "    N = X.shape[0]\n",
    "    s = 0\n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi = y[i]\n",
    "        \n",
    "        #Prediction Function\n",
    "        u = beta @ xiHat\n",
    "        yi_pred = softmax(u)\n",
    "        \n",
    "        #Calculating the loss\n",
    "        s += multiclass_cross_entropy(yi,yi_pred)\n",
    "    \n",
    "    #Returns the average loss \n",
    "    return s / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform Multiclass Logistic Regression using Stochastic Gradient Descent\n",
    "def multiLogReg_SGD(X,y,batch,a_val,ep): #(Training, Testing, Batch_Size, Alpha, Epoch)\n",
    "    num_epochs = ep\n",
    "    alpha = a_val\n",
    "    N, d = X.shape\n",
    "    X = np.insert(X,0,1,axis=1) #Inserting Leading 1's to create augmented matrix\n",
    "    K = y.shape[1] #Setting K to the number of classes in the test set\n",
    "    \n",
    "    batch_size = batch #Initialising batch_size \n",
    "    \n",
    "    beta = np.zeros((K,d+1)) #Initialising beta vector\n",
    "    L_vals = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        L_val = L(beta,X,y) #Calculate the loss for every epoch\n",
    "        L_vals.append(L_val) #Add the losses into L_vals\n",
    "        \n",
    "        print('| Epoch : ' + str(epoch) + ' | Loss : ' + str(L_val))\n",
    "        \n",
    "        prm = np.random.permutation(N) #A random permutation of size N \n",
    "        \n",
    "        batch_idx = 0\n",
    "        for start_idx in range(0,N,batch_size): #Starting from 0, to length N in steps of the batch size\n",
    "            \n",
    "            stop_idx = start_idx + batch_size \n",
    "            stop_idx = min(stop_idx, N)\n",
    "            \n",
    "            num_examples_in_batch = stop_idx - start_idx \n",
    "        \n",
    "            for i in prm[start_idx:stop_idx]:\n",
    "                xiHat = X[i]\n",
    "                yi = y[i]\n",
    "                u = beta @ xiHat\n",
    "                \n",
    "                #Prediction Function\n",
    "                yi_pred = softmax(u)\n",
    "                \n",
    "                #Calculating gradient \n",
    "                grad = np.outer(yi_pred - yi, xiHat)\n",
    "\n",
    "            #Divide gradient by number of examples in batch \n",
    "            grad = grad / num_examples_in_batch\n",
    "            beta = beta - alpha*grad \n",
    "            \n",
    "            batch_idx += 1\n",
    "            \n",
    "    \n",
    "    return beta, L_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict labels\n",
    "def predictLabels(beta,X):\n",
    "    X = np.insert(X,0,1,axis = 1)\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        xiHat = X[i]\n",
    "        yi_pred = softmax(beta @ xiHat)\n",
    "        \n",
    "        k = np.argmax(yi_pred)\n",
    "        predictions.append(k)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate accuracy\n",
    "def accuracy_score(predicted_labels,truth_labels):\n",
    "    \n",
    "    yi_pred = np.array(predicted_labels)\n",
    "    truth = np.array(truth_labels)\n",
    "    num_correct = sum(p == t for p, t in zip(yi_pred, truth)) \n",
    "    accuracy = num_correct/truth_labels.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the training set \n",
    "N_train, numRows, numCols = X_train.shape\n",
    "X_train = np.reshape(X_train,(N_train,numRows*numCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encoding\n",
    "y_train = pd.get_dummies(y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch : 0 | Loss : 2.3025850929954172\n",
      "| Epoch : 1 | Loss : 0.7478375617837253\n",
      "| Epoch : 2 | Loss : 0.575040907389319\n",
      "| Epoch : 3 | Loss : 0.507214506652957\n",
      "| Epoch : 4 | Loss : 0.4688793489334052\n",
      "| Epoch : 5 | Loss : 0.44434912608452726\n",
      "| Epoch : 6 | Loss : 0.42621315180443736\n",
      "| Epoch : 7 | Loss : 0.4109275839995608\n",
      "| Epoch : 8 | Loss : 0.39901411138491233\n",
      "| Epoch : 9 | Loss : 0.3911669494263595\n",
      "| Epoch : 10 | Loss : 0.38406908033427917\n",
      "| Epoch : 11 | Loss : 0.3747312106627982\n",
      "| Epoch : 12 | Loss : 0.3693341036614135\n",
      "| Epoch : 13 | Loss : 0.3636067732590776\n",
      "| Epoch : 14 | Loss : 0.35898253871933455\n",
      "| Epoch : 15 | Loss : 0.35554483817212135\n",
      "| Epoch : 16 | Loss : 0.3525378087765864\n",
      "| Epoch : 17 | Loss : 0.3485334629427758\n",
      "| Epoch : 18 | Loss : 0.3444823664924098\n",
      "| Epoch : 19 | Loss : 0.3413492470289609\n",
      "| Epoch : 20 | Loss : 0.3386846220285912\n",
      "| Epoch : 21 | Loss : 0.33732613891612\n",
      "| Epoch : 22 | Loss : 0.3343641516373868\n",
      "| Epoch : 23 | Loss : 0.3348793184604953\n",
      "| Epoch : 24 | Loss : 0.32955142623426287\n",
      "| Epoch : 25 | Loss : 0.3289198308107813\n",
      "| Epoch : 26 | Loss : 0.3266709500002594\n",
      "| Epoch : 27 | Loss : 0.3259241719946902\n",
      "| Epoch : 28 | Loss : 0.32341473249517994\n",
      "| Epoch : 29 | Loss : 0.3226584466042396\n",
      "| Epoch : 30 | Loss : 0.32067661357465604\n",
      "| Epoch : 31 | Loss : 0.3189262605345473\n",
      "| Epoch : 32 | Loss : 0.31925763001927693\n",
      "| Epoch : 33 | Loss : 0.31681213133442254\n",
      "| Epoch : 34 | Loss : 0.3181684843878479\n",
      "| Epoch : 35 | Loss : 0.3160352142447122\n",
      "| Epoch : 36 | Loss : 0.31358475963342086\n",
      "| Epoch : 37 | Loss : 0.3122492875085403\n",
      "| Epoch : 38 | Loss : 0.31036336746609666\n",
      "| Epoch : 39 | Loss : 0.31081538473669157\n",
      "| Epoch : 40 | Loss : 0.30870427613101964\n",
      "| Epoch : 41 | Loss : 0.30821320295297056\n",
      "| Epoch : 42 | Loss : 0.3068438485754055\n",
      "| Epoch : 43 | Loss : 0.30650469304077205\n",
      "| Epoch : 44 | Loss : 0.3055330549072814\n",
      "| Epoch : 45 | Loss : 0.305248140025417\n",
      "| Epoch : 46 | Loss : 0.3048651422531071\n",
      "| Epoch : 47 | Loss : 0.30379950804886713\n",
      "| Epoch : 48 | Loss : 0.30307640543239006\n",
      "| Epoch : 49 | Loss : 0.3021047487442962\n",
      "| Epoch : 50 | Loss : 0.30166903147478247\n",
      "| Epoch : 51 | Loss : 0.3014803445305876\n",
      "| Epoch : 52 | Loss : 0.3002100443445028\n",
      "| Epoch : 53 | Loss : 0.30012333025163096\n",
      "| Epoch : 54 | Loss : 0.300644667415156\n",
      "| Epoch : 55 | Loss : 0.2988497276904872\n",
      "| Epoch : 56 | Loss : 0.2969973587903459\n",
      "| Epoch : 57 | Loss : 0.2974092521202329\n",
      "| Epoch : 58 | Loss : 0.2967103616620002\n",
      "| Epoch : 59 | Loss : 0.29576720901353226\n",
      "| Epoch : 60 | Loss : 0.2958585278227695\n",
      "| Epoch : 61 | Loss : 0.2960065364553129\n",
      "| Epoch : 62 | Loss : 0.29513579258295264\n",
      "| Epoch : 63 | Loss : 0.29404142097506575\n",
      "| Epoch : 64 | Loss : 0.29359332562483886\n",
      "| Epoch : 65 | Loss : 0.29435706247636245\n",
      "| Epoch : 66 | Loss : 0.29289738408861565\n",
      "| Epoch : 67 | Loss : 0.29258620556073905\n",
      "| Epoch : 68 | Loss : 0.292368475154728\n",
      "| Epoch : 69 | Loss : 0.2911624812782648\n",
      "| Epoch : 70 | Loss : 0.29159534154694855\n",
      "| Epoch : 71 | Loss : 0.2921768650599624\n",
      "| Epoch : 72 | Loss : 0.28988555689180395\n",
      "| Epoch : 73 | Loss : 0.28962177607581674\n",
      "| Epoch : 74 | Loss : 0.28885680255599794\n",
      "| Epoch : 75 | Loss : 0.2898430587043812\n",
      "| Epoch : 76 | Loss : 0.2886598323521995\n",
      "| Epoch : 77 | Loss : 0.28908322237941864\n",
      "| Epoch : 78 | Loss : 0.28997607177449153\n",
      "| Epoch : 79 | Loss : 0.2871018432849232\n",
      "| Epoch : 80 | Loss : 0.2877592147401198\n",
      "| Epoch : 81 | Loss : 0.2867380250350817\n",
      "| Epoch : 82 | Loss : 0.2857957455774665\n",
      "| Epoch : 83 | Loss : 0.2861167923313673\n",
      "| Epoch : 84 | Loss : 0.28592610966385\n",
      "| Epoch : 85 | Loss : 0.2851318021866659\n",
      "| Epoch : 86 | Loss : 0.28602833010182177\n",
      "| Epoch : 87 | Loss : 0.28474379416684065\n",
      "| Epoch : 88 | Loss : 0.28494730987681616\n",
      "| Epoch : 89 | Loss : 0.28450563895956416\n",
      "| Epoch : 90 | Loss : 0.2837887767394487\n",
      "| Epoch : 91 | Loss : 0.28324216753824794\n",
      "| Epoch : 92 | Loss : 0.28355740230268633\n",
      "| Epoch : 93 | Loss : 0.2840880238995029\n",
      "| Epoch : 94 | Loss : 0.2835781981934434\n",
      "| Epoch : 95 | Loss : 0.28243593276792456\n",
      "| Epoch : 96 | Loss : 0.2823992500376367\n",
      "| Epoch : 97 | Loss : 0.2820689482789143\n",
      "| Epoch : 98 | Loss : 0.2826020876322936\n",
      "| Epoch : 99 | Loss : 0.28292044165445474\n"
     ]
    }
   ],
   "source": [
    "beta, L_vals = multiLogReg_SGD(X_train,y_train,10,0.01,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe402d25a50>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the test array\n",
    "N_test = X_test.shape[0]\n",
    "X_test = np.reshape(X_test,(N_test,numRows*numCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions for X_test\n",
    "predictions = predictLabels(beta,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4, 1, 4, 9, 6, 9]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 10 rows of the predictions\n",
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 10 rows of the truth\n",
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy : \n",
    "accuracy_score(predictions,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
